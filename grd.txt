from guardrails import Guard
from openai import OpenAI
import os

# Load Guard schema for US financial compliance
guard = Guard.from_rail_file("planner_rail.xml")

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

user_input = "The system failed after a $9000 wire transfer flagged account 847211 with potential fraud activity"

output, history = guard(
    prompt="What is the next action in this observability issue: " + user_input,
    llm_api=client.chat.completions.create,
    prompt_params={
        "model": "gpt-4",
        "messages": [{"role": "user", "content": user_input}]
    }
)

print("Validated Response:", output)




<rail version="0.1">
<output>
    <string name="action" required="true"/>
    <string name="target"/>
    <string name="reason"/>
</output>

<validators>
    <regex name="no_sensitive_info" match="^(?!.*\\b(ssn|account|routing|credit|card|transfer|fraud|balance|wire|email|phone|address|zip|dob|social security|ach|transaction)\\b).*$"
        on="reason"
        description="No U.S. financial PII or sensitive terms allowed in 'reason'." />
</validators>
</rail>





PErfect code

import os
from langchain.chat_models import ChatOpenAI
from langchain.agents import initialize_agent, AgentType
from langchain.tools import tool
from guardrails import Guard

# === Step 1: Define Tools ===
@tool
def add(x: int, y: int) -> int:
    """Add two numbers together."""
    return x + y

@tool
def multiply(x: int, y: int) -> int:
    """Multiply two numbers together."""
    return x * y

@tool
def divide(x: int, y: int) -> float:
    """Divide x by y (returns float)."""
    if y == 0:
        return "Cannot divide by zero."
    return x / y

tools = [add, multiply, divide]

# === Step 2: Define Guardrails RAIL schema ===
RAIL_SCHEMA = """
<rail version="0.1">
<output>
    <string name="action" required="true" />
    <string name="result" required="true" />
</output>

<validators>
    <regex name="no_sensitive_words"
           match="^(?!.*\\b(password|ssn|account|secret)\\b).*$"
           on="result"
           description="No sensitive info allowed in result." />
</validators>
</rail>
"""

# === Step 3: Create Guard Object ===
guard = Guard.from_rail_string(RAIL_SCHEMA)

# === Step 4: LangChain LLM and Agent Setup ===
llm = ChatOpenAI(model="gpt-4", temperature=0.2, openai_api_key=os.getenv("OPENAI_API_KEY"))

agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# === Step 5: User Prompt ===
user_prompt = "Add 10 and 20, then multiply the result by 3. Finally, divide it by 2."

# === Step 6: LangChain Agent Output ===
agent_response = agent.run(user_prompt)

# === Step 7: Validate Output with Guardrails ===
# Simulate structured response from agent
model_output = f"""
action: math_chain
result: {agent_response}
"""

output, history = guard(
    prompt="Summarize tool use result in JSON format.",
    output=model_output
)

print("\n‚úÖ Final Validated Output:", output)



pip install langchain openai guardrails-ai
export OPENAI_API_KEY=sk-xxxxx
python secure_tool_agent.py




Complete Gaurd Rail

import os
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from guardrails import Guard
from openai import OpenAI

# === 1. Define RAIL Schema ===
RAIL_SCHEMA = """
<rail version="0.1">
<output>
    <string name="action" required="true"/>
    <string name="result" required="true"/>
</output>

<validators>
    <regex name="no_sensitive_words"
           match="^(?!.*\\b(password|ssn|account|secret|routing|credit card)\\b).*$"
           on="result"
           description="No sensitive info allowed in result." />
</validators>
</rail>
"""

guard = Guard.from_rail_string(RAIL_SCHEMA)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# === 2. LangChain Prompt Template ===
template = PromptTemplate.from_template(
    "Given the task: {task}, break it into action steps and return only the final result."
)

task = "Add 100 and 200, then divide the result by 3."

# === 3. Apply Guardrails to LLM Response ===
final_prompt = template.format(task=task)

output, history = guard(
    prompt=final_prompt,
    llm_api=client.chat.completions.create,
    prompt_params={
        "model": "gpt-4",
        "messages": [{"role": "user", "content": final_prompt}]
    }
)

# === 4. Print Output ===
print("üîê Guardrails-Protected Output from LangChain Prompt:")
print(output)




Use the below code for now



import os
from langchain.chat_models import ChatOpenAI
from langchain.agents import initialize_agent, AgentType
from langchain.tools import tool
from guardrails import Guard
from openai import OpenAI

# === Define Guardrails Schema ===
RAIL_SCHEMA = """
<rail version="0.1">
<output>
    <string name="action" required="true"/>
    <string name="result" required="true"/>
</output>

<validators>
    <regex name="no_sensitive_words"
           match="^(?!.*\\b(password|ssn|account|secret|credit card|routing|token)\\b).*$"
           on="result"
           description="No sensitive info allowed in result." />
</validators>
</rail>
"""

guard = Guard.from_rail_string(RAIL_SCHEMA)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# === Define LangChain Tools ===
@tool
def add(x: int, y: int) -> int:
    """Add two numbers together."""
    return x + y

@tool
def multiply(x: int, y: int) -> int:
    """Multiply two numbers together."""
    return x * y

@tool
def divide(x: int, y: int) -> float:
    """Divide x by y (returns float)."""
    if y == 0:
        return "Cannot divide by zero."
    return x / y

tools = [add, multiply, divide]

# === Initialize LangChain Agent ===
llm = ChatOpenAI(model="gpt-4", temperature=0.1, openai_api_key=os.getenv("OPENAI_API_KEY"))

agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# === User Prompt and Agent Reasoning ===
prompt = "Add 20 and 30, then multiply the result by 2. What is the final result?"

agent_result = agent.run(prompt)

# === Guardrails Validation ===
guard_input = f"""
action: math_tool_chain
result: {agent_result}
"""

output, history = guard(
    prompt="Validate structured agent output with safety checks.",
    output=guard_input
)

# === Final Output ===
print("‚úÖ Validated & Structured Output:")
print(output)


Single Agent Example

import os
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# === Sample input from perceptive agent ===
server_data = [
    {"server": "srv1", "over_util_reason": "CPU"},
    {"server": "srv2", "over_util_reason": "memory"},
    {"server": "srv3", "over_util_reason": "both"},
]

# Convert to string for LLM
input_summary = "\n".join(
    f"{item['server']}: over-utilized due to {item['over_util_reason']}" for item in server_data
)

# === Prompt ===
prompt_template = PromptTemplate(
    input_variables=["status"],
    template="""
You are an observability assistant. Given the server over-utilization status below, summarize it clearly for a non-technical stakeholder:

{status}

Respond in 2‚Äì3 sentences, in plain English.
"""
)

# === LLM Setup ===
llm = ChatOpenAI(
    model="gpt-4",
    temperature=0.3,
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

chain = LLMChain(llm=llm, prompt=prompt_template)

# === Run ===
response = chain.run({"status": input_summary})

print("\nüß† User-Friendly Summary:\n", response)

Same agent with security

import os
from guardrails import Guard
from openai import OpenAI

# === Step 1: Define Guardrails Schema ===
RAIL_SCHEMA = """
<rail version="0.1">
<output>
    <string name="summary" required="true"/>
</output>

<validators>
    <regex name="no_sensitive_terms"
           match="^(?!.*\\b(ssn|account|password|secret|confidential|customer)\\b).*$"
           on="summary"
           description="Do not include sensitive terms in the output." />
</validators>
</rail>
"""

guard = Guard.from_rail_string(RAIL_SCHEMA)

# === Step 2: Setup OpenAI Client ===
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# === Step 3: Input from Perceptive Agent ===
server_data = [
    {"server": "srv1", "over_util_reason": "CPU"},
    {"server": "srv2", "over_util_reason": "memory"},
    {"server": "srv3", "over_util_reason": "both"},
]

# Convert to readable string
input_summary = "\n".join(
    f"{item['server']}: over-utilized due to {item['over_util_reason']}" for item in server_data
)

# === Step 4: Prompt ===
prompt = f"""
You are an observability assistant. Given the server over-utilization status below, summarize it clearly for a non-technical stakeholder:

{input_summary}

Respond in 2‚Äì3 sentences, in plain English. Do not use any terms that could imply customer data, financial info, or credentials.
"""

# === Step 5: Guardrails-Validated LLM Call ===
output, history = guard(
    prompt=prompt,
    llm_api=client.chat.completions.create,
    prompt_params={
        "model": "gpt-4",
        "messages": [{"role": "user", "content": prompt}]
    }
)

# === Step 6: Final Output ===
print("‚úÖ User-Safe Summary:\n", output["summary"])
