Train Model


import os
import numpy as np
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler
import joblib

# Simulate training data for multiple servers
np.random.seed(42)
servers = ["server1", "server2", "server3"]
data_per_server = {
    "server1": np.random.normal(200, 20, (1000, 4)),  # 4 metrics
    "server2": np.random.normal(150, 30, (1000, 4)),
    "server3": np.random.normal(100, 15, (1000, 4))
}

# Create a folder to save models
model_folder = "models"
os.makedirs(model_folder, exist_ok=True)

for server, data in data_per_server.items():
    # Normalize the data
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)
    
    # Train One-Class SVM
    model = OneClassSVM(kernel='rbf', nu=0.05, gamma=0.1)
    model.fit(data_scaled)
    
    # Save the model and scaler
    model_path = os.path.join(model_folder, f"{server}_model.pkl")
    scaler_path = os.path.join(model_folder, f"{server}_scaler.pkl")
    joblib.dump(model, model_path)
    joblib.dump(scaler, scaler_path)
    
    print(f"Saved model and scaler for {server} to {model_folder}/")

execute model

def load_model_and_predict(server, input_data, model_folder="shared_models"):
    """
    Load the model and scaler for a given server and predict anomalies on the input data.
    
    Args:
        server (str): Name of the server (e.g., "server1").
        input_data (numpy array): Data to predict anomalies on.
        model_folder (str): Folder where models and scalers are stored.
    
    Returns:
        numpy array: Predictions (+1 for normal, -1 for anomaly).
    """
    import joblib
    import os
    
    # Paths to the model and scaler
    model_path = os.path.join(model_folder, f"{server}_model.pkl")
    scaler_path = os.path.join(model_folder, f"{server}_scaler.pkl")
    
    # Load the model and scaler
    if not os.path.exists(model_path) or not os.path.exists(scaler_path):
        raise FileNotFoundError(f"Model or scaler for {server} not found in {model_folder}")
    
    model = joblib.load(model_path)
    scaler = joblib.load(scaler_path)
    
    # Normalize the input data using the loaded scaler
    input_data_scaled = scaler.transform(input_data)
    
    # Predict anomalies
    predictions = model.predict(input_data_scaled)
    decision_scores = model.decision_function(input_data_scaled)
    
    return predictions, decision_scores


# Simulate new data for server1
new_data = np.random.normal(200, 20, (10, 4))  # 10 new samples for 4 metrics

# Specify the shared folder where models are stored
shared_folder = "models"  # This could be a shared network folder

# Predict using the model for server1
server_name = "server1"
predictions, scores = load_model_and_predict(server_name, new_data, model_folder=shared_folder)

print(f"Predictions for {server_name}: {predictions}")
print(f"Decision Scores for {server_name}: {scores}")


synthetic data

