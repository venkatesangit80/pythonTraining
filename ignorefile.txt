Train Model


import os
import numpy as np
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler
import joblib

# Simulate training data for multiple servers
np.random.seed(42)
servers = ["server1", "server2", "server3"]
data_per_server = {
    "server1": np.random.normal(200, 20, (1000, 4)),  # 4 metrics
    "server2": np.random.normal(150, 30, (1000, 4)),
    "server3": np.random.normal(100, 15, (1000, 4))
}

# Create a folder to save models
model_folder = "models"
os.makedirs(model_folder, exist_ok=True)

for server, data in data_per_server.items():
    # Normalize the data
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(data)
    
    # Train One-Class SVM
    model = OneClassSVM(kernel='rbf', nu=0.05, gamma=0.1)
    model.fit(data_scaled)
    
    # Save the model and scaler
    model_path = os.path.join(model_folder, f"{server}_model.pkl")
    scaler_path = os.path.join(model_folder, f"{server}_scaler.pkl")
    joblib.dump(model, model_path)
    joblib.dump(scaler, scaler_path)
    
    print(f"Saved model and scaler for {server} to {model_folder}/")

execute model

def load_model_and_predict(server, input_data, model_folder="shared_models"):
    """
    Load the model and scaler for a given server and predict anomalies on the input data.
    
    Args:
        server (str): Name of the server (e.g., "server1").
        input_data (numpy array): Data to predict anomalies on.
        model_folder (str): Folder where models and scalers are stored.
    
    Returns:
        numpy array: Predictions (+1 for normal, -1 for anomaly).
    """
    import joblib
    import os
    
    # Paths to the model and scaler
    model_path = os.path.join(model_folder, f"{server}_model.pkl")
    scaler_path = os.path.join(model_folder, f"{server}_scaler.pkl")
    
    # Load the model and scaler
    if not os.path.exists(model_path) or not os.path.exists(scaler_path):
        raise FileNotFoundError(f"Model or scaler for {server} not found in {model_folder}")
    
    model = joblib.load(model_path)
    scaler = joblib.load(scaler_path)
    
    # Normalize the input data using the loaded scaler
    input_data_scaled = scaler.transform(input_data)
    
    # Predict anomalies
    predictions = model.predict(input_data_scaled)
    decision_scores = model.decision_function(input_data_scaled)
    
    return predictions, decision_scores


# Simulate new data for server1
new_data = np.random.normal(200, 20, (10, 4))  # 10 new samples for 4 metrics

# Specify the shared folder where models are stored
shared_folder = "models"  # This could be a shared network folder

# Predict using the model for server1
server_name = "server1"
predictions, scores = load_model_and_predict(server_name, new_data, model_folder=shared_folder)

print(f"Predictions for {server_name}: {predictions}")
print(f"Decision Scores for {server_name}: {scores}")


synthetic data

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

# Function to generate synthetic data
def generate_synthetic_data(start_time, end_time, sampling_rate_minutes, anomaly_rate=0.01):
    """
    Generate synthetic data for response time, throughput, error count, and stall count.
    
    Args:
        start_time (str): Start time in "YYYY-MM-DD HH:MM" format.
        end_time (str): End time in "YYYY-MM-DD HH:MM" format.
        sampling_rate_minutes (int): Sampling rate in minutes (e.g., 1 or 5).
        anomaly_rate (float): Fraction of points that will be anomalous.
    
    Returns:
        pd.DataFrame: Synthetic data with timestamp and metrics.
    """
    # Generate timestamps
    start = datetime.strptime(start_time, "%Y-%m-%d %H:%M")
    end = datetime.strptime(end_time, "%Y-%m-%d %H:%M")
    timestamps = [start + timedelta(minutes=i) for i in range(0, int((end - start).total_seconds() / 60), sampling_rate_minutes)]
    
    # Generate normal data
    response_time = np.random.normal(loc=200, scale=20, size=len(timestamps))  # ms
    throughput = np.random.normal(loc=50, scale=5, size=len(timestamps))       # req/sec
    error_count = np.random.normal(loc=0.01, scale=0.005, size=len(timestamps))  # ratio
    stall_count = np.random.normal(loc=2, scale=0.5, size=len(timestamps))      # stalls/sec
    
    # Introduce anomalies
    num_anomalies = int(len(timestamps) * anomaly_rate)
    anomaly_indices = np.random.choice(len(timestamps), num_anomalies, replace=False)
    response_time[anomaly_indices] *= np.random.uniform(1.5, 3, size=num_anomalies)  # Spike
    throughput[anomaly_indices] *= np.random.uniform(0.5, 0.8, size=num_anomalies)  # Drop
    error_count[anomaly_indices] *= np.random.uniform(2, 5, size=num_anomalies)    # Spike
    stall_count[anomaly_indices] *= np.random.uniform(1.5, 3, size=num_anomalies)  # Spike
    
    # Create DataFrame
    data = {
        "timestamp": timestamps,
        "response_time": response_time,
        "throughput": throughput,
        "error_count": error_count,
        "stall_count": stall_count,
    }
    df = pd.DataFrame(data)
    return df

# Generate synthetic data
start_time = "2024-12-01 00:00"
end_time = "2024-12-01 23:59"

# Generate data for 1-minute and 5-minute intervals
data_1_minute = generate_synthetic_data(start_time, end_time, sampling_rate_minutes=1)
data_5_minutes = generate_synthetic_data(start_time, end_time, sampling_rate_minutes=5)

# Save to CSV files
data_1_minute.to_csv("synthetic_data_1_minute.csv", index=False)
data_5_minutes.to_csv("synthetic_data_5_minutes.csv", index=False)

# Display the first few rows of 1-minute data
print(data_1_minute.head())

# Plot synthetic data (response time example)
plt.figure(figsize=(10, 6))
plt.plot(data_1_minute["timestamp"], data_1_minute["response_time"], label="Response Time (1 min)", alpha=0.7)
plt.scatter(
    data_1_minute["timestamp"][data_1_minute["response_time"] > 400], 
    data_1_minute["response_time"][data_1_minute["response_time"] > 400], 
    color="red", label="Anomalies (Response Time)"
)
plt.title("Synthetic Response Time Data with Anomalies")
plt.xlabel("Time")
plt.ylabel("Response Time (ms)")
plt.legend()
plt.grid()
plt.show()






from unittest.mock import patch, MagicMock
import your_module  # Replace with your actual module name

def test_your_function():
    with patch('your_module.DatabaseConnection') as MockConnection:
        mock_conn = MockConnection.return_value
        mock_cursor = mock_conn.cursor.return_value

        # Configure the mock cursor's executemany method
        mock_cursor.executemany.return_value = None

        # Define your expected SQL query and parameters
        expected_query = "INSERT INTO your_table (column1, column2) VALUES (:1, :2)"
        expected_params = [('value1', 'value2'), ('value3', 'value4')]

        # Call the function you're testing
        your_module.your_function()

        # Assert that executemany was called with the expected query and parameters
        mock_cursor.executemany.assert_called_once_with(expected_query, expected_params)





import { format } from 'date-fns';
import { utcToZonedTime } from 'date-fns-tz';

const convertToEST = (dateString) => {
  const date = new Date(dateString); // UTC Date
  const timeZone = 'America/New_York'; // EST time zone
  const zonedDate = utcToZonedTime(date, timeZone); // Convert to EST
  return format(zonedDate, 'yyyy-MM-dd hh:mm:ss aaaa'); // Format output
};

// Example usage
const inputDate = '2025-01-13T15:30:00Z'; // UTC time
console.log(convertToEST(inputDate)); // Output: "2025-01-13 10:30:00 AM"



Time Zone,Date and Time
IST,January 13, 2025, 12:00 AM
UTC,January 12, 2025, 6:30 PM
EST,January 13, 2025, 12:00 AM
UTC,January 13, 2025, 5:00 AM



Packet Statistics:
	•	node_network_receive_packets_total: Total packets received.
	•	node_network_transmit_packets_total: Total packets transmitted.


100 * ((rate(node_network_receive_bytes_total[5m]) + rate(node_network_transmit_bytes_total[5m])) * 8) / <NIC_SPEED_IN_BPS>

100 * ((rate(node_network_receive_bytes_total[5m]) + rate(node_network_transmit_bytes_total[5m])) * 8) / 1000000000


100 * (rate(node_network_transmit_bytes_total[5m]) * 8) / <NIC_SPEED_IN_BPS>